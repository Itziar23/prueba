{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz6FZS6ylds5IQ5n/jnnBp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Itziar23/prueba/blob/master/Copia_de_Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JuKY1Fh6jIX"
      },
      "source": [
        "# Implementation of LadderNet using the tf.keras Functional API\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras import backend as keras\n",
        "\n",
        "drop = 0.25\n",
        "\n",
        "def ResBlock(input_tensor, filters):\n",
        "    \"\"\"\"\n",
        "    Define Residual Block showed in Zhuang (2019)\n",
        "    CONV => BATCH => RELU => DROPOUT => CONV => BATCH\n",
        "      |                                   |\n",
        "      |----------- SHARED LAYER ----------|\n",
        "      \n",
        "    \"\"\"\n",
        "        \n",
        "    conv_1 = Conv2D(filters = filters, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')    \n",
        "    conv_1a = conv_1(input_tensor) # Shared weights conv layer\n",
        "    batch_1 = BatchNormalization()(conv_1a)\n",
        "    relu_1  = Activation(\"relu\")(batch_1)\n",
        "    drop_1  = Dropout(drop)(relu_1)\n",
        "    conv_1b = conv_1(drop_1) # Shared weights conv layer\n",
        "    batch_1 = BatchNormalization()(conv_1b)\n",
        "    return batch_1\n",
        "\n",
        "def LadderNet(input_size = (256, 256, 1), num_classes=2, filters=30):\n",
        "    \n",
        "    \"\"\"\n",
        "    LadderNet (Zhuang, 2019) implementation in tensorflow.keras\n",
        "    Method: Keras Functional API\n",
        "    \"\"\"  \n",
        "    \n",
        "    # X's denote standard flow\n",
        "    # XNUM denote ResBlock outputs\n",
        "    \n",
        "    # \"First\" UNet\n",
        "    \n",
        "    # Input branch\n",
        "    inputs = Input(input_size)\n",
        "    X = Conv2D(filters=filters, kernel_size=3, activation=\"relu\", padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "\n",
        "    # Down branch\n",
        "    X1 = ResBlock(input_tensor=X, filters=filters) # ResBlock located in the first layer of the paper scheme\n",
        "    X = Conv2D(filters=filters*2, kernel_size=3, strides=2, kernel_initializer='he_normal')(X1) \n",
        "    X = Activation(\"relu\")(X) # This ReLU is not shown in the paper scheme\n",
        "    \n",
        "    X2 = ResBlock(input_tensor=X, filters=filters*2)\n",
        "    X = Conv2D(filters=filters*4, kernel_size=3, strides=2, kernel_initializer='he_normal')(X2)\n",
        "    X = Activation(\"relu\")(X)\n",
        "    \n",
        "    X3 = ResBlock(input_tensor=X, filters=filters*4)\n",
        "    X = Conv2D(filters=filters*8, kernel_size=3, strides=2, kernel_initializer='he_normal')(X3)\n",
        "    X = Activation(\"relu\")(X)\n",
        "    \n",
        "    X4 = ResBlock(input_tensor=X, filters=filters*8)\n",
        "    X = Conv2D(filters=filters*16, kernel_size=3, strides=2, kernel_initializer='he_normal')(X4)\n",
        "    X = Activation(\"relu\")(X)\n",
        "    \n",
        "    # Bottom block \n",
        "    X = ResBlock(input_tensor=X, filters=filters*16)\n",
        "    \n",
        "    # Up branch\n",
        "    X = Conv2DTranspose(filters=filters*8, kernel_size=3, strides=2, kernel_initializer='he_normal')(X)\n",
        "    X = Add()([X, X4])\n",
        "    # X = Activation(\"relu\")(X) # This ReLU is commented in the paper code\n",
        "    X5 = ResBlock(input_tensor=X, filters=filters*8)\n",
        "    \n",
        "    X = Conv2DTranspose(filters=filters*4, kernel_size=3, strides=2, kernel_initializer='he_normal')(X5)\n",
        "    X = Add()([X, X3])\n",
        "    # X = Activation(\"relu\")(X)\n",
        "    X6 = ResBlock(input_tensor=X, filters=filters*4)\n",
        "    \n",
        "    X = Conv2DTranspose(filters=filters*2, kernel_size=3, strides=2, kernel_initializer='he_normal')(X6)\n",
        "    X = Add()([X, X2])\n",
        "    # X = Activation(\"relu\")(X)\n",
        "    X7 = ResBlock(input_tensor=X, filters=filters*2)\n",
        "        \n",
        "    X = Conv2DTranspose(filters=filters, kernel_size=3, strides=2, output_padding=1, kernel_initializer='he_normal')(X7)\n",
        "    X = Add()([X, X1])\n",
        "    # X = Activation(\"relu\")(X)\n",
        "    X = ResBlock(input_tensor=X, filters=filters)\n",
        "    \n",
        "    # Top block (bottle-neck)\n",
        "    X8 = ResBlock(input_tensor=X, filters=filters)\n",
        "    X = ResBlock(input_tensor=X, filters=filters)\n",
        "    X = Add()([X, X8])\n",
        "    \n",
        "    # \"Second\" UNet\n",
        "    \n",
        "    # Down branch\n",
        "    X9 = ResBlock(input_tensor=X, filters=filters)\n",
        "    X = Conv2D(filters=filters*2, kernel_size=3, strides=2, kernel_initializer='he_normal')(X) \n",
        "    X = Activation(\"relu\")(X)\n",
        "    X = Add()([X7, X])    \n",
        "    \n",
        "    X10 = ResBlock(input_tensor=X, filters=filters*2)\n",
        "    X = Conv2D(filters=filters*4, kernel_size=3, strides=2, kernel_initializer='he_normal')(X)    \n",
        "    X = Activation(\"relu\")(X)    \n",
        "    X = Add()([X6, X])\n",
        "    \n",
        "    X11 = ResBlock(input_tensor=X, filters=filters*4)\n",
        "    X = Conv2D(filters=filters*8, kernel_size=3, strides=2, kernel_initializer='he_normal')(X)    \n",
        "    X = Activation(\"relu\")(X)\n",
        "    X = Add()([X5, X])\n",
        "\n",
        "    X12 = ResBlock(input_tensor=X, filters=filters*8)\n",
        "    X = Conv2D(filters=filters*16, kernel_size=3, strides=2, kernel_initializer='he_normal')(X)    \n",
        "    X = Activation(\"relu\")(X)\n",
        "    \n",
        "    # Bottom block\n",
        "    X = ResBlock(input_tensor=X, filters=filters*16)\n",
        "    \n",
        "    # Up branch\n",
        "    X = Conv2DTranspose(filters=filters*8, kernel_size=3, strides=2, kernel_initializer='he_normal')(X)\n",
        "    X = Add()([X, X12])   \n",
        "    # X = Activation(\"relu\")(X)\n",
        "    X = ResBlock(input_tensor=X, filters=filters*8)\n",
        "    \n",
        "    X = Conv2DTranspose(filters=filters*4, kernel_size=3, strides=2, kernel_initializer='he_normal')(X)\n",
        "    X = Add()([X, X11])\n",
        "    # X = Activation(\"relu\")(X)\n",
        "    X = ResBlock(input_tensor=X, filters=filters*4)\n",
        "    \n",
        "    X = Conv2DTranspose(filters=filters*2, kernel_size=3, strides=2, kernel_initializer='he_normal')(X)\n",
        "    X = Add()([X, X10])\n",
        "    # X = Activation(\"relu\")(X)\n",
        "    X = ResBlock(input_tensor=X, filters=filters*2)\n",
        "    \n",
        "    X = Conv2DTranspose(filters=filters, kernel_size=3, strides=2, kernel_initializer='he_normal', output_padding=1)(X)\n",
        "    X = Add()([X, X9])\n",
        "    # X = Activation(\"relu\")(X)\n",
        "    X = ResBlock(input_tensor=X, filters=filters)\n",
        "    \n",
        "    # Final block\n",
        "    X = Conv2D(filters=num_classes, kernel_size=1, kernel_initializer='he_normal')(X)\n",
        "    # X = Activation(\"relu\")(X)\n",
        "    X = Activation(\"softmax\")(X)\n",
        "    #X = Conv2D(1, 1)(X)\n",
        "    \n",
        "    model = Model(inputs, X)\n",
        "    \n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "def conv2D_module(inputs, filters, kernel_size=3, padding=\"valid\", pool_size=2):\n",
        "    \n",
        "    \"\"\"\n",
        "    CONV => RELU => CONV => RELU => MAXPOOL\n",
        "    \"\"\"\n",
        "    \n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding,\n",
        "               kernel_initializer='he_normal')(inputs)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding,\n",
        "               kernel_initializer='he_normal')(inputs)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "\n",
        "def UNet(input_size, depth, num_classes, filters, batch_norm):\n",
        "    \n",
        "    \"\"\"\n",
        "    UNet (Ronneberger, 2015) implementation in tensorflow.keras\n",
        "    using Keras Functional API.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Input layer\n",
        "    inputs = Input(input_size)\n",
        "    x = inputs\n",
        "    \n",
        "    # Encoding\n",
        "    down_list = []\n",
        "    for layer in range(depth):\n",
        "        x = Conv2D(filters, 3, activation='relu', padding='same', kernel_initializer='he_normal')(x)\n",
        "        \n",
        "        if batch_norm: \n",
        "            x = BatchNormalization()(x)\n",
        "            x = Conv2D(filters, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "            x_down = BatchNormalization()(x)\n",
        "        else:\n",
        "            x_down = Conv2D(filters, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "        \n",
        "        down_list.append(x_down)\n",
        "        x = MaxPooling2D(pool_size=2)(x_down)\n",
        "        filters = filters*2\n",
        "    \n",
        "    # Bottom\n",
        "    x = Conv2D(filters, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "    if batch_norm: x = BatchNormalization()(x)\n",
        "    x = Conv2D(filters, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "    if batch_norm: x = BatchNormalization()(x)\n",
        "    \n",
        "    # Decoding\n",
        "    for layer in reversed(down_list):\n",
        "        filters = filters // 2\n",
        "        x = UpSampling2D((2,2))(x)\n",
        "        x = concatenate([x, layer])\n",
        "        x = Conv2D(filters, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "        if batch_norm: x = BatchNormalization()(x)\n",
        "        x = Conv2D(filters, 3, padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "        if batch_norm: x = BatchNormalization()(x)\n",
        "    \n",
        "    # Output layer\n",
        "    x = Conv2D(filters=num_classes, kernel_size=1)(x)\n",
        "    if batch_norm: x = BatchNormalization()(x)\n",
        "    outputs = Activation(\"softmax\")(x)\n",
        "    \n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "    \n",
        "    \"\"\"\n",
        "    conv1_1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1_2 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = Conv2D(4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    outputs = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    #model.summary()\n",
        "    if(pretrained_weights):\n",
        "    \tmodel.load_weights(pretrained_weights)\n",
        "    return model\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "AAF1MQQTkn4r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}